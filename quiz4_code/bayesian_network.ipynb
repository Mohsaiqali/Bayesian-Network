{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54785ec9",
   "metadata": {},
   "source": [
    "# Influence of energy consumption on greenhouse gases in Turkey\n",
    "\n",
    "This exercise uses a Bayesian Network to model the influence of energy consumption on greenhouse gases in Turkey. It has been adapted from ``pgmpy`` [Bayesian Network tutorial](https://pgmpy.org/detailed_notebooks/11.%20A%20Bayesian%20Network%20to%20model%20the%20influence%20of%20energy%20consumption%20on%20greenhouse%20gases%20in%20Italy.html) by Lorenzo Mario Amorosa, Alma Mater Studiorum – Università di Bologna (Italy).\n",
    "\n",
    "We will analyze the effects of variations in different growth indicators on greenhouse gases via energy consumption. We are given the causal relations graph, which is the directed acyclic graph of the Bayesian Network. We will construct the network and learn its parameters from data. Then, we will use this Bayesian Network model to obtain probabilistic results about greenhouse gases given some evidence.\n",
    "\n",
    "This exercise uses [Bayesian Network model](https://pgmpy.org/models/bayesiannetwork.html) in ``pgmpy``."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126090a9",
   "metadata": {},
   "source": [
    "## Constructing the network\n",
    "\n",
    "We will use the following variables in our model construction with the hypothesis that growth rates of population, urbanization, and gross domestic product (GDP) affect energy use in a country, and energy use affects the emission of greenhouse gases.\n",
    "\n",
    "* POP = Population growth (annual %)\n",
    "* URB = Urban population growth (annual %)\n",
    "* GDP = GDP per capita growth (annual %)\n",
    "* EC = Energy use (kg of oil equivalent per capita) - [annual growth %]\n",
    "* FFEC = Fossil fuel energy consumption (% of total) - [annual growth %]\n",
    "* REC = Renewable energy consumption (% of total final energy consumption) - [annual growth %]\n",
    "* EI = Energy imports, net (% of energy use) - [annual growth %]\n",
    "* CO2 = CO2 emissions (metric tons per capita) - [annual growth %]\n",
    "* CH4 = Methane emissions in energy sector (thousand metric tons of CO2 equivalent) - [annual growth %]\n",
    "* N2O = Nitrous oxide emissions in energy sector (thousand metric tons of CO2 equivalent) - [annual growth %]\n",
    "\n",
    "The causal relations between the variables are shown in the below graph.\n",
    "\n",
    "<img src=\"images/BN.png\" alt=\"Bayesian Network\" width=\"400\">\n",
    "\n",
    "Please construct the graph using ``BayesianNetwork`` model in ``pgmpy``. (You can check the API in here: https://pgmpy.org/models/bayesiannetwork.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebed56bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.models import BayesianNetwork\n",
    "\n",
    "# please provide the directed edges as a list of tuples\n",
    "# be careful about edge directions!\n",
    "model = BayesianNetwork([('POP', 'EC'), ('URB','EC' ), ('GDP','EC'),('EC','FFEC'),('EC','REC'),('EC','EI'),('FFEC','CO2'),('FFEC','CH4'),('FFEC','N20'),('REC','CH4'),('REC','N20'),('REC','CO2')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcfdc0e",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The data has been collected from [the databank of The World Bank](https://databank.worldbank.org). We will compute conditional probability tables (CPTs) of the Bayesian Network using this data. Before moving into CPT calculation, we need to prepare the data. Some variables in the dataset are not in annual growth format, we will convert them to annual growth percentages and append \" - [annual growth %]\" to variable names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2df7b5",
   "metadata": {},
   "source": [
    "### Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314b74c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv, DataFrame\n",
    "import numpy as np\n",
    "\n",
    "def annual_growth(row, years):\n",
    "    min_year = years[\"min\"]\n",
    "    max_year = years[\"max\"]\n",
    "    row[\"Series Name\"] = row[\"Series Name\"] + \" - [annual growth %]\"\n",
    "    for year in range(max_year, min_year, -1):\n",
    "        if not np.isnan(row[str(year)]) and not np.isnan(row[str(year - 1)]):\n",
    "            row[str(year)] = 100 * (float(row[str(year)]) - float(row[str(year - 1)])) / abs(float(row[str(year - 1)]))\n",
    "        else:\n",
    "            row[str(year)] = np.nan\n",
    "    row[str(min_year)] = np.nan\n",
    "    return row\n",
    "\n",
    "years = {\"min\" : 1960, \"max\" : 2021}\n",
    "df_raw = read_csv(\"raw_data.csv\")\n",
    "df_raw_growth = DataFrame(data=[row if \"growth\" in row[\"Series Name\"] else annual_growth(row, years) for index, row in df_raw.iterrows()])\n",
    "print(\"There are \" + str(df_raw_growth.shape[0]) + \" indicators in the dataframe.\")\n",
    "df_raw_growth.head(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caf0ecb",
   "metadata": {},
   "source": [
    "### Data cleaning\n",
    "\n",
    "This step drops the unused columns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19c9cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = ['POP', 'URB', 'GDP', 'EC', 'FFEC', 'REC', 'EI', 'CO2', 'CH4', 'N2O']\n",
    "df_growth = df_raw_growth.transpose().iloc[4:]\n",
    "df_growth.columns = nodes\n",
    "df_growth.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714428fe",
   "metadata": {},
   "source": [
    "### Data discretization\n",
    "\n",
    "We will also discretize the values of variables by using binning. Low, medium, and large are binned into 'A', 'B', and 'C', respectively.\n",
    "\n",
    "*Please note that there are some missing values, but ``pgmpy`` can handle missing data!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7ad237",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TIERS_NUM = 3\n",
    "\n",
    "def boundary_str(start, end, tier):\n",
    "#     return f'{tier}: {start:+0,.2f} to {end:+0,.2f}'\n",
    "    return f'{tier}'\n",
    "\n",
    "def print_boundaries(label, boundaries):\n",
    "    print('{} - A: {:.2f} to {:.2f}, B: {:.2f} to {:.2f}, C: {:.2f} to {:.2f}'.format(label,boundaries[0][0], boundaries[0][1],boundaries[1][0], boundaries[1][1],boundaries[2][0], boundaries[2][1]))\n",
    "\n",
    "def relabel(v, boundaries):\n",
    "    if v >= boundaries[0][0] and v <= boundaries[0][1]:\n",
    "        return boundary_str(boundaries[0][0], boundaries[0][1], tier='A')\n",
    "    elif v >= boundaries[1][0] and v <= boundaries[1][1]:\n",
    "        return boundary_str(boundaries[1][0], boundaries[1][1], tier='B')\n",
    "    elif v >= boundaries[2][0] and v <= boundaries[2][1]:\n",
    "        return boundary_str(boundaries[2][0], boundaries[2][1], tier='C')\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def get_boundaries(tiers):\n",
    "    prev_tier = tiers[0]\n",
    "    boundaries = [(prev_tier[0], prev_tier[prev_tier.shape[0] - 1])]\n",
    "    for index, tier in enumerate(tiers):\n",
    "        if index is not 0:\n",
    "            boundaries.append((prev_tier[prev_tier.shape[0] - 1], tier[tier.shape[0] - 1]))\n",
    "            prev_tier = tier\n",
    "    return boundaries\n",
    "\n",
    "new_columns = {}\n",
    "for i, content in enumerate(df_growth.items()):\n",
    "    (label, series) = content\n",
    "    values = np.sort(np.array([x for x in series.tolist() if not np.isnan(x)] , dtype=float))\n",
    "    if values.shape[0] < TIERS_NUM:\n",
    "        print(f'Error: there are not enough data for label {label}')\n",
    "        break\n",
    "    boundaries = get_boundaries(tiers=np.array_split(values, TIERS_NUM))\n",
    "    print_boundaries(label, boundaries)\n",
    "    new_columns[label] = [relabel(value, boundaries) for value in series.tolist()]\n",
    "\n",
    "df = DataFrame(data=new_columns)\n",
    "df.columns = nodes\n",
    "df.index = range(years[\"min\"], years[\"max\"] + 1)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee44bb21",
   "metadata": {},
   "source": [
    "## Learning network parameters\n",
    "\n",
    "In this part, we will learn CPTs of the Bayesian Network. We will use ``model.fit()`` with ``BayesianEstimator`` and ``MaximumLikelihoodEstimator`` from ``pgmpy.estimators``. We can specify if we will learn CPTs using only complete data (``complete_samples_only=False``) or using all data with some missing values (``complete_samples_only=True``).\n",
    "\n",
    "*Please note that ``BayesianEstimator`` requires a prior type to use for the model parameters. We will use ``prior_type=\"BDeu\"`` with ``equivalent_sample_size=10`` ([see details in here](https://pgmpy.org/param_estimator/bayesian_est.html)).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735f0133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.estimators import BayesianEstimator, MaximumLikelihoodEstimator\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# disable text wrapping in output cell\n",
    "display(HTML(\"<style>div.output_area pre {white-space: pre;}</style>\"))\n",
    "\n",
    "model.cpds = []\n",
    "model.fit(data=df,\n",
    "          estimator=BayesianEstimator,\n",
    "          prior_type=\"BDeu\",\n",
    "          equivalent_sample_size=10,\n",
    "          complete_samples_only=False)\n",
    "\n",
    "print(f'Check model: {model.check_model()}\\n')\n",
    "for cpd in model.get_cpds():\n",
    "    print(f'CPT of {cpd.variable}:')\n",
    "    print(cpd,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517fc6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POP(B)\n",
    "print('POP(B): {:.3f}'.format(model.get_cpds(\"POP\").values[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b06494",
   "metadata": {},
   "source": [
    "## Network analysis\n",
    "\n",
    "### Some definitions\n",
    "\n",
    "**In a Bayesian Network:**\n",
    "\n",
    "* A variable is conditionally independent of its **non-descendants**, given its parents\n",
    "* A variable is conditionally independent of all other nodes in the network, given its parents, children, and children's parents - that is given its **Markov blanket**.\n",
    "* Given three nodes $X$, $Y$, and $Z$, when influence can flow from $X$ to $Y$ via $Z$, we say that the **trail** $X \\rightleftharpoons Z \\rightleftharpoons Y$ is **active**. There are for possible active two-edge trails:\n",
    "    * **Causal trail** $X \\rightarrow Z \\rightarrow Y$: active if and only if $Z$ is not observed\n",
    "    * **Evidential trail** $X \\leftarrow Z \\leftarrow Y$: active if and only if $Z$ is not observed\n",
    "    * **Common cause** $X \\leftarrow Z \\rightarrow Y$: active if and only if $Z$ is not observed\n",
    "    * **Common effect** $X \\rightarrow Z \\leftarrow Y$: active if and only if either $Z$ or one of $Z$'s descendants is observed\n",
    "\n",
    "Let's consider a longer trail $X_1 \\rightleftharpoons \\cdots \\rightleftharpoons X_n$. Intiutively, for influence to \"flow\" from $X_1$ to $X_n$, it needs to flow through every single node on the trail. In other words, $X_1$ can influence $X_n$ if every two-edge trail $X_{i-1} \\rightleftharpoons X_i \\rightleftharpoons X_{i+1}$ along the trail allows influence to flow.\n",
    "\n",
    "Let $\\mathcal{G}$ be a Bayesian Network structure and $\\mathbf{X}$, $\\mathbf{Y}$, $\\mathbf{Z}$ be three sets of nodes in $\\mathcal{G}$. We say that $\\mathbf{X}$ and $\\mathbf{Y}$ are **d-seperated** given $\\mathbf{Z}$, denoted $\\text{d-sep}_{\\mathcal{G}}(\\mathbf{X};\\mathbf{Y}|\\mathbf{Z})$, if there is no active trail between any node $X \\in \\mathbf{X}$ and $Y \\in \\mathbf{Y}$ given $\\mathbf{Z}$.\n",
    "\n",
    "### Task\n",
    "This section analyzes the network in terms of independencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce009c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There can be made {len(model.get_independencies().get_assertions())}',\n",
    "      'valid independence assertions with respect to the all possible given evidence.')\n",
    "print('For instance, any node in the network is independent of its non-descendents given its parents (local semantics):\\n',\n",
    "      f'\\n{model.local_independencies(nodes)}\\n')\n",
    "\n",
    "def active_trails_of(query, evidence):\n",
    "    active = model.active_trail_nodes(query, observed=evidence).get(query)\n",
    "    active.remove(query)\n",
    "    if active:\n",
    "        if evidence:\n",
    "            print(f'Active trails between \\'{query}\\' and {active} given the evidence {set(evidence)}.')\n",
    "        else:\n",
    "            print(f'Active trails between \\'{query}\\' and {active} given no evidence.')\n",
    "    else:\n",
    "        print(f'No active trails for \\'{query}\\' given the evidence {set(evidence)}.')\n",
    "\n",
    "def markov_blanket_of(node):\n",
    "    print(f'Markov blanket of \\'{node}\\' is {set(model.get_markov_blanket(node))}')\n",
    "\n",
    "active_trails_of(query='FFEC', evidence=[])\n",
    "active_trails_of(query='FFEC', evidence=['EC'])\n",
    "active_trails_of(query='FFEC', evidence=['EC', 'CO2'])\n",
    "active_trails_of(query='EI', evidence=['EC'])\n",
    "active_trails_of(query='CH4', evidence=['EC', 'FFEC'])\n",
    "print()\n",
    "markov_blanket_of(node='POP')\n",
    "markov_blanket_of(node='EC')\n",
    "markov_blanket_of(node='REC')\n",
    "markov_blanket_of(node='CH4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f82e73",
   "metadata": {},
   "source": [
    "### Some queries on independence assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66364008",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.independencies.Independencies import IndependenceAssertion\n",
    "\n",
    "def check_assertion(model, independent, from_variables, evidence):\n",
    "    assertion = IndependenceAssertion(independent, from_variables, evidence)\n",
    "    result = False\n",
    "    for a in model.get_independencies().get_assertions():\n",
    "        if frozenset(assertion.event1) == a.event1 and assertion.event2 <= a.event2 and frozenset(assertion.event3) == a.event3:\n",
    "            result = True\n",
    "            break\n",
    "    print(f'{assertion}: {result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ea3cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_assertion(model, independent=['EI'], from_variables=['N2O'], evidence=['EC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ae9139",
   "metadata": {},
   "source": [
    "## Inferences\n",
    "\n",
    "In this part, we will make inferences using variable elimination over the Bayesian Network. We will use ``VariableElimination`` in ``pgmpy``'s inference module ([see details in here](https://pgmpy.org/exact_infer/ve.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35508bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.inference import VariableElimination\n",
    "import time\n",
    "\n",
    "def query_report(infer, variables, evidence=None, elimination_order=\"MinFill\", show_progress=False, desc=\"\"):\n",
    "    if desc:\n",
    "        print(desc)\n",
    "    start_time = time.time()\n",
    "    print(infer.query(variables=variables,\n",
    "                      evidence=evidence,\n",
    "                      elimination_order=elimination_order,\n",
    "                      show_progress=show_progress))\n",
    "    print(f'--- Query executed in {time.time() - start_time:0,.4f} seconds ---\\n')\n",
    "\n",
    "infer = VariableElimination(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4606b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = ['CO2']\n",
    "ev = {'EC': 'A'}\n",
    "query_report(infer, variables=var, evidence=ev, desc=f'Probability query of {var} with evidence {ev}:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dbf6bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
